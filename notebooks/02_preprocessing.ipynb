{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82676edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Preprocessing of Software Requirements\n",
    "Dataset source: https://data.mendeley.com/datasets/4ysx9fyzv4/1\n",
    "\n",
    "Steps:\n",
    "1. Read original dataset (Functional/Non-Functional requirements)\n",
    "2. Clean, normalize and lemmatize text using spaCy\n",
    "3. Remove duplicates and missing entries\n",
    "4. Split into training and test sets\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    ROOT = Path(__file__).resolve().parents[1]\n",
    "except NameError:\n",
    "    ROOT = Path.cwd().parent\n",
    "\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.append(str(ROOT))\n",
    "\n",
    "from config import DATA_RAW, DATA_PROCESSED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "976a0fa9-8feb-4171-b598-6e37392d9482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy English model (disable parser and NER to speed up)\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "678ad41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset: 6117 rows\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "file_path = DATA_RAW / \"FR_NFR_Dataset.xlsx\"\n",
    "if not file_path.exists():\n",
    "    raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "\n",
    "df = pd.read_excel(DATA_RAW / \"FR_NFR_Dataset.xlsx\", engine='openpyxl')\n",
    "df.rename(columns={\"Requirement Text\": \"requirement\"}, inplace=True)\n",
    "\n",
    "print(f\"Original dataset: {df.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9054ccae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure column consistency\n",
    "if \"Type\" in df.columns:\n",
    "    df.rename(columns={\"Type\": \"label\"}, inplace=True)\n",
    "\n",
    "if \"requirement\" not in df.columns or \"label\" not in df.columns:\n",
    "    raise ValueError(\"Dataset must contain 'requirement' and 'label' columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72f77bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove missing or duplicated entries\n",
    "df.dropna(subset=[\"requirement\", \"label\"], inplace=True)\n",
    "df.drop_duplicates(subset=[\"requirement\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da8cdce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning and lemmatizing text: 100%|██████████| 5977/5977 [00:20<00:00, 285.22it/s]\n"
     ]
    }
   ],
   "source": [
    "# Text preprocessing with spaCy\n",
    "\n",
    "def clean_text_spacy(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Lowercase, remove punctuation, stopwords, and lemmatize using spaCy.\n",
    "    Only keeps alphabetic tokens.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \" \", text)  # keep letters and numbers\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "tqdm.pandas(desc=\"Cleaning and lemmatizing text\")\n",
    "df[\"clean_text\"] = df[\"requirement\"].astype(str).progress_apply(clean_text_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eced6c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any empty clean_text entries\n",
    "df = df[df[\"clean_text\"].str.strip() != \"\"]\n",
    "\n",
    "# Split into train/test\n",
    "train_df, test_df = train_test_split(\n",
    "    df, test_size=0.2, random_state=42, stratify=df[\"label\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1990f7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed data saved to: ../data/processed\n",
      "Train set: 4781 rows\n",
      "Test set:  1196 rows\n",
      "\n",
      "Class distribution:\n",
      " label\n",
      "FR     3931\n",
      "NFR    2046\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Save processed data\n",
    "train_df.to_csv(DATA_PROCESSED / \"train.csv\", index=False)\n",
    "test_df.to_csv(DATA_PROCESSED / \"test.csv\", index=False)\n",
    "\n",
    "# Summary\n",
    "print(f\"\\nProcessed data saved to: {DATA_PROCESSED}\")\n",
    "print(f\"Train set: {train_df.shape[0]} rows\")\n",
    "print(f\"Test set:  {test_df.shape[0]} rows\")\n",
    "print(\"\\nClass distribution:\\n\", df[\"label\"].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab_npl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
