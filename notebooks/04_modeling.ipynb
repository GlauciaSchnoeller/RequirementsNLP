{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3186bc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Training and evaluating models for FR/NFR classification\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ") # Fine-tune BERT with HuggingFace\n",
    "\n",
    "try:\n",
    "    ROOT = Path(__file__).resolve().parents[1]\n",
    "except NameError:\n",
    "    ROOT = Path.cwd().parent\n",
    "\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.append(str(ROOT))\n",
    "\n",
    "from config import DATA_PROCESSED, MODELS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc682aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv(DATA_PROCESSED / \"train.csv\")\n",
    "test_df = pd.read_csv(DATA_PROCESSED / \"test.csv\")\n",
    "\n",
    "y_train = train_df[\"label\"]\n",
    "y_test = test_df[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58449bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load precomputed features\n",
    "X_train_tfidf = np.load(DATA_PROCESSED / \"X_train_tfidf.npy\")\n",
    "X_test_tfidf = np.load(DATA_PROCESSED / \"X_test_tfidf.npy\")\n",
    "\n",
    "X_train_bert = np.load(DATA_PROCESSED / \"X_train_bert.npy\") if (DATA_PROCESSED / \"X_train_bert.npy\").exists() else None\n",
    "X_test_bert = np.load(DATA_PROCESSED / \"X_test_bert.npy\") if (DATA_PROCESSED / \"X_test_bert.npy\").exists() else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a18de31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM...\n",
      "Results (SVM - TF-IDF):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          FR       0.89      0.93      0.91       787\n",
      "         NFR       0.86      0.79      0.82       409\n",
      "\n",
      "    accuracy                           0.88      1196\n",
      "   macro avg       0.88      0.86      0.87      1196\n",
      "weighted avg       0.88      0.88      0.88      1196\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/home/glaucia/RequirementsNLP/models/svm_tfidf.pkl']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train SVM\n",
    "print(\"Training SVM...\")\n",
    "svm_clf = SVC(kernel=\"linear\", probability=True)\n",
    "svm_clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred_svm = svm_clf.predict(X_test_tfidf)\n",
    "\n",
    "print(\"Results (SVM - TF-IDF):\")\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "\n",
    "joblib.dump(svm_clf, MODELS_DIR / \"svm_tfidf.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2301560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression (TF-IDF)...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          FR       0.87      0.95      0.90       787\n",
      "         NFR       0.88      0.72      0.79       409\n",
      "\n",
      "    accuracy                           0.87      1196\n",
      "   macro avg       0.87      0.83      0.85      1196\n",
      "weighted avg       0.87      0.87      0.87      1196\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/home/glaucia/RequirementsNLP/models/log_reg_tfidf.pkl']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train Logistic Regression\n",
    "print(\"Training Logistic Regression (TF-IDF)...\")\n",
    "log_reg = LogisticRegression(max_iter=200, solver=\"liblinear\")\n",
    "log_reg.fit(X_train_tfidf, y_train)\n",
    "y_pred_lr = log_reg.predict(X_test_tfidf)\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "joblib.dump(log_reg, MODELS_DIR / \"log_reg_tfidf.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2da8111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MLP (BERT embeddings)...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          FR       0.86      0.93      0.89       787\n",
      "         NFR       0.84      0.71      0.77       409\n",
      "\n",
      "    accuracy                           0.86      1196\n",
      "   macro avg       0.85      0.82      0.83      1196\n",
      "weighted avg       0.85      0.86      0.85      1196\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/glaucia/miniconda3/envs/lab_npl/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Train MLP with BERT embeddings\n",
    "if X_train_bert is not None:\n",
    "    print(\"Training MLP (BERT embeddings)...\")\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(256,), activation=\"relu\", max_iter=30, random_state=42)\n",
    "    mlp.fit(X_train_bert, y_train)\n",
    "    y_pred_mlp = mlp.predict(X_test_bert)\n",
    "    print(classification_report(y_test, y_pred_mlp))\n",
    "    joblib.dump(mlp, MODELS_DIR / \"mlp_bert.pkl\")\n",
    "else:\n",
    "    print(\"BERT embeddings not found — skipping MLP model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee00a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/glaucia/miniconda3/envs/lab_npl/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Map: 100%|██████████| 4781/4781 [00:00<00:00, 6058.03 examples/s]\n",
      "Map: 100%|██████████| 1196/1196 [00:00<00:00, 4694.17 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_531436/1893141231.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/home/glaucia/miniconda3/envs/lab_npl/lib/python3.12/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='897' max='897' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [897/897 2:32:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.320217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.377900</td>\n",
       "      <td>0.305366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.377900</td>\n",
       "      <td>0.333691</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/glaucia/miniconda3/envs/lab_npl/lib/python3.12/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/glaucia/miniconda3/envs/lab_npl/lib/python3.12/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Convert labels to integers for HF Trainer\n",
    "label2id = {label: idx for idx, label in enumerate(train_df['label'].unique())}\n",
    "train_df['label'] = train_df['label'].map(label2id)\n",
    "test_df['label'] = test_df['label'].map(label2id)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df[[\"clean_text\", \"label\"]])\n",
    "test_dataset = Dataset.from_pandas(test_df[[\"clean_text\", \"label\"]])\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"clean_text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "train_tokenized = train_dataset.map(tokenize_function, batched=True)\n",
    "test_tokenized = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(label2id)\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"models/bert_finetuned\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=test_tokenized,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"models/bert_finetuned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f48d6e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed successfully!\n",
      "Saved models:\n",
      "- mlp_bert.pkl\n",
      "- log_reg_tfidf.pkl\n",
      "- svm_tfidf.pkl\n"
     ]
    }
   ],
   "source": [
    "print(\"Training completed successfully!\")\n",
    "print(\"Saved models:\")\n",
    "for f in MODELS_DIR.iterdir():\n",
    "    print(\"-\", f.name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab_npl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
